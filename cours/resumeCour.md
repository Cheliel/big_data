


# ğŸ·ï¸ Les Types de Variables en Machine Learning  

---

## ğŸ”¹ 1. Variables Qualitatives  

ğŸ“Œ **DÃ©finition :**  
Les **variables qualitatives** (ou **catÃ©goriques**) reprÃ©sentent des **informations non numÃ©riques** qui dÃ©crivent une caractÃ©ristique.  

ğŸ“Œ **Exemples :**  
- **Genre** : Homme / Femme / Autre  
- **Couleur prÃ©fÃ©rÃ©e** : Rouge, Bleu, Vert  
- **Type de produit** : ThÃ©, CafÃ©  

---

## ğŸ”¹ 2. Variables Ordinales  

ğŸ“Œ **DÃ©finition :**  
Les **variables ordinales** sont un **sous-type de variables qualitatives**, mais elles ont **un ordre logique** entre elles.  

ğŸ“Œ **Exemples :**  
- **Niveau dâ€™Ã©ducation** : Primaire < CollÃ¨ge < LycÃ©e < UniversitÃ©  
- **Satisfaction client** : Mauvais < Moyen < Bon < Excellent  
- **Taille de vÃªtements** : S < M < L < XL  

---

## ğŸ”¹ 3. Variables Quantitatives  

ğŸ“Œ **DÃ©finition :**  
Les **variables quantitatives** sont **des nombres** et peuvent Ãªtre **mesurÃ©es ou comptÃ©es**.  

ğŸ“Œ **Deux types principaux :**  

### âœ… **Variables Quantitatives DiscrÃ¨tes**  
âœ” Valeurs **finies et comptables** (pas de dÃ©cimales).  
âœ” **Exemples** :  
  - Nombre de tasses de cafÃ© bues par jour â˜•  
  - Nombre d'enfants dans une famille ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦  
  - Nombre de produits vendus ğŸ›ï¸  

### ğŸ“ **Variables Quantitatives Continues**  
âœ” **Peuvent prendre une infinitÃ© de valeurs dans un intervalle donnÃ©** (dÃ©cimales possibles).  
âœ” **Exemples** :  
  - **Taille** (ex: 1,75m) ğŸ“  
  - **Poids** (ex: 72,5 kg) âš–ï¸  
  - **Ã‚ge** (ex: 25,3 ans) ğŸ‚  

---

ğŸ¯ **RÃ©sumÃ© :**  
- **Qualitatif** â†’ CatÃ©gories sans ordre (ex: Couleurs, Produits).  
- **Ordinal** â†’ CatÃ©gories avec ordre (ex: Satisfaction, Ã‰ducation).  
- **Quantitatif Discret** â†’ Nombre entier comptable (ex: Nombre de cafÃ©s).  
- **Quantitatif Continu** â†’ Mesurable avec dÃ©cimales (ex: Taille, Poids).  


# ğŸ”¢ Quand les donnÃ©es quantitatives ne sont pas comparables  

---

## ğŸ“Œ 1ï¸âƒ£ Centrage et RÃ©duction  

> **Objectif :** Permet de rendre les variables quantitatives comparables, en particulier pour les algorithmes utilisant la **descente de gradient**.  

ğŸ“Œ **Pourquoi c'est nÃ©cessaire ?**  
- Certains algorithmes sont sensibles aux **Ã©carts dâ€™Ã©chelle** entre les variables (ex: un poids en kg et une distance en km).  
- **Exemple** : Comparer **les choux ğŸ¥¬ et les carottes ğŸ¥•** nÃ©cessite une transformation pour ramener toutes les variables Ã  une Ã©chelle commune.  
- **Solution** : **Centrer** (moyenne = 0) et **rÃ©duire** (Ã©cart-type = 1) les variables quantitatives.  

---

## ğŸ“Œ 2ï¸âƒ£ Analyse en Composantes Principales (ACP)  

> **Objectif :** RÃ©duire la dimensionnalitÃ© dâ€™un jeu de donnÃ©es en **conservant un maximum dâ€™information**.  

ğŸ“Œ **Important Ã  savoir** :  
âœ” **Lâ€™ACP ne fonctionne que sur les variables quantitatives.**  
âœ” Elle repose sur des **calculs de moyenne, mÃ©diane, Ã©cart-type**, qui **nâ€™ont aucun sens mathÃ©matique pour les variables qualitatives**.  
âœ” Elle permet de **transformer un grand nombre de variables corrÃ©lÃ©es** en **un petit nombre de nouvelles variables indÃ©pendantes**.  

ğŸ“Š **Exemple d'utilisation :**  
- Simplifier **un grand tableau de donnÃ©es clients** en quelques axes principaux.  
- Faciliter la **visualisation et lâ€™interprÃ©tation des donnÃ©es**.  

---

ğŸ¯ **RÃ©sumÃ© :**  
- **Centrage & RÃ©duction** â†’ Indispensable pour rendre les variables quantitatives comparables, surtout avec des algorithmes basÃ©s sur la **descente de gradient**.  
- **ACP** â†’ Utile pour **rÃ©duire la dimensionnalitÃ© des donnÃ©es**, mais **uniquement applicable aux variables quantitatives**.  

# ğŸ­ Quand les Variables sont Qualitatives  

---

## ğŸ“Œ 1ï¸âƒ£ Transformer les ModalitÃ©s en Valeurs NumÃ©riques  

> **Objectif :** Convertir les variables qualitatives en **valeurs exploitables** par les algorithmes de Machine Learning.  

ğŸ“Œ **Exemple : Moyen de paiement, Ã©tat de santÃ©**  
- Certaines variables comme **le jour de la semaine** peuvent Ãªtre codÃ©es avec des valeurs numÃ©riques (**1 Ã  7** pour Lundi Ã  Dimanche).  
- **Attention âš ï¸** : Ces valeurs **nâ€™ont pas de signification mathÃ©matique** !  
  - **Jour 1 et Jour 7 ne sont pas plus proches que Jour 1 et Jour 3**.  
  - **(1 - 7) â‰  (15 - 22)** â†’ Ce sont juste des **Ã©tiquettes numÃ©riques** et non des valeurs ordonnÃ©es.  

---

## ğŸ“Œ 2ï¸âƒ£ Transformation des ModalitÃ©s en **One-Hot Encoding (Binarisation)**  

> **Objectif :** ReprÃ©senter chaque modalitÃ© comme une **colonne binaire** pour Ã©viter les erreurs dâ€™interprÃ©tation des valeurs numÃ©riques.  

ğŸ“Œ **MÃ©thode :**  
- Chaque modalitÃ© devient une **colonne distincte**.  
- La valeur est **1** si la modalitÃ© est prÃ©sente, **0** sinon.  

ğŸ“Š **Exemple : Jour de la Semaine**  
| Jour | Lundi | Mardi | Mercredi | Jeudi | Vendredi | Samedi | Dimanche |
|------|-------|-------|----------|-------|----------|--------|---------|
| Mercredi | 0 | 0 | **1** | 0 | 0 | 0 | 0 |
| Samedi | 0 | 0 | 0 | 0 | 0 | **1** | 0 |

âœ” **Cette technique est appelÃ©e One-Hot Encoding et permet aux modÃ¨les dâ€™apprentissage de traiter les variables qualitatives sans biais.**  

---

ğŸ¯ **RÃ©sumÃ© :**  
- **Conversion en valeurs numÃ©riques** â†’ UtilisÃ© pour **reprÃ©senter des catÃ©gories** (ex: jours de la semaine), mais **sans signification mathÃ©matique**.  
- **One-Hot Encoding** â†’ Technique qui transforme chaque **modalitÃ© en colonne binaire** (ex: un jour spÃ©cifique devient une colonne "Samedi" avec des valeurs **0/1**).  





# Les typologie d'algorithme 

## ğŸ§  Apprentissage Non SupervisÃ©

    Lâ€™apprentissage non supervisÃ© permet dâ€™identifier des groupes dâ€™individus similaires sans avoir de classes dÃ©finies Ã  lâ€™avance.


### ğŸ“Œ Pourquoi lâ€™utiliser ?

- Quand on a un jeu de **donnÃ©es sans Ã©tiquettes et quâ€™on veut regrouper** des Ã©lÃ©ments similaires.
- Quand on veut faire du **clustering** (ex: segmentation de clients).
- Quand on veut rÃ©duire la dimension des donnÃ©es pour optimiser un modÃ¨le.


### ğŸ“Œ Exemples concrets :

- Un opÃ©rateur tÃ©lÃ©com analyse les achats de ses clients pour proposer des publicitÃ©s ciblÃ©es.
- Une banque dÃ©tecte des transactions frauduleuses grÃ¢ce aux comportements anormaux.
- Un site e-commerce classe ses produits en fonction des prÃ©fÃ©rences des clients.

### ğŸ“Œ Transition vers lâ€™apprentissage supervisÃ© :

- Une fois les donnÃ©es regroupÃ©es ou Ã©tiquetÃ©es, on peut rÃ©utiliser ces informations dans un modÃ¨le supervisÃ©.
- Cela permet dâ€™amÃ©liorer la prÃ©cision dâ€™un modÃ¨le de classification en exploitant les nouveaux groupes dÃ©tectÃ©s.


### ğŸ“Œ Les algorithmes non supervisÃ©s

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">MÃ©thode</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Utilisation principale</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Exemples concrets</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">Clustering (K-Means, DBSCAN)</td>
            <td style="padding: 10px; border: 1px solid #555;">Regrouper des individus selon des similaritÃ©s</td>
            <td style="padding: 10px; border: 1px solid #555;">Segmentation clients, regroupement dâ€™images</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">RÃ©duction de dimension (PCA, Auto-encodeurs)</td>
            <td style="padding: 10px; border: 1px solid #555;">RÃ©duire le nombre de variables sans perdre trop dâ€™info</td>
            <td style="padding: 10px; border: 1px solid #555;">Compression dâ€™images, visualisation des donnÃ©es</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">DÃ©tection dâ€™anomalies</td>
            <td style="padding: 10px; border: 1px solid #555;">Identifier des valeurs atypiques</td>
            <td style="padding: 10px; border: 1px solid #555;">DÃ©tection de fraudes bancaires, pannes machines</td>
        </tr>
    </tbody>
</table>



## SupervisÃ© 

Il y a une colonne X ou Y a expliquer.  

Il y deux type d'algo : 

- **RÃ©gression** Y est *quantitatif*

- **Classification** binaire Y est *qualitatif* 


Une modalitÃ© c'est la reprÃ©sentation de toute les valeur possible d'une valeur qualitative par Vrai / Faux. En somme on crÃ©e une colonne par valeur possible (vert / jaune / rouge) et chaque colonne rÃ©pond par Vrai ou Faux. Chaque colonne est une modalitÃ©. 



## ğŸ§  Apprentissage SupervisÃ©

    Lâ€™apprentissage supervisÃ© est une mÃ©thode oÃ¹ le modÃ¨le apprend Ã  partir de donnÃ©es Ã©tiquetÃ©es pour prÃ©dire une variable cible Y en fonction des caractÃ©ristiques X.

### ğŸ“Œ ModalitÃ©s et transformation des donnÃ©es qualitatives :

- Une modalitÃ© est une valeur possible dâ€™une variable qualitative (ex: "Rouge", "Vert", "Bleu").
- On utilise One-Hot Encoding pour convertir ces modalitÃ©s en colonnes binaires (1 = vrai, 0 = faux).
- Cela permet aux algorithmes dâ€™utiliser les donnÃ©es catÃ©gorielles efficacement.

### ğŸ“Œ Types dâ€™algorithmes supervisÃ©s

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Type</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Explication</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Exemples</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">RÃ©gression</td>
            <td style="padding: 10px; border: 1px solid #555;">UtilisÃ©e quand <b>Y est une variable continue (quantitative)</b>.</td>
            <td style="padding: 10px; border: 1px solid #555;">PrÃ©diction du prix dâ€™une maison, estimation du temps de trajet.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Classification</td>
            <td style="padding: 10px; border: 1px solid #555;">UtilisÃ©e quand <b>Y est une variable discrÃ¨te (qualitative)</b>.</td>
            <td style="padding: 10px; border: 1px solid #555;">Classification dâ€™un email (Spam/Non Spam), reconnaissance dâ€™images (Chat/Chien).</td>
        </tr>
    </tbody>
</table>



# ğŸ§© Algorithmes Non SupervisÃ©s  
---
## ğŸŒ³ Classification HiÃ©rarchique Ascendante (CHA)  

La **Classification HiÃ©rarchique Ascendante (CHA)** est un **algorithme de clustering** qui regroupe des individus en **fonction de leur similaritÃ©**. Il se base sur **la distance entre les diffÃ©rents individus** pour construire un **arbre de regroupement** (**dendrogramme**).  

ğŸ“Œ **Pourquoi utiliser CHA ?**  
- Permet de **visualiser les relations entre les groupes**.  
- Utile pour **dÃ©terminer le nombre optimal de clusters**.  
- AdaptÃ© aux **petits jeux de donnÃ©es** oÃ¹ les autres mÃ©thodes de clustering peuvent Ãªtre moins efficaces.  

---

## ğŸ”— 1ï¸âƒ£ Algorithme **AglomÃ©ratif** (Regroupement Progressif)  

> **Approche ascendante** : On commence avec chaque individu comme un **groupe distinct**, puis on les **fusionne progressivement** en fonction de leur proximitÃ©.  

ğŸ“Œ **Principe :**  
1. On cherche les **deux individus ou groupes les plus proches**.  
2. On les **fusionne** en un seul cluster.  
3. On rÃ©pÃ¨te lâ€™opÃ©ration jusqu'Ã  obtenir **un seul groupe englobant tout le dataset**.  

ğŸ“ **InterprÃ©tation du dendrogramme :**  
- **La hauteur des branches reprÃ©sente la diffÃ©rence entre les clusters.**  
- **Plus une branche est haute, plus les groupes sont dissemblables.**  
- **On peut couper lâ€™arbre Ã  diffÃ©rentes hauteurs** pour dÃ©terminer le **nombre optimal de clusters**.  

ğŸ’¡ **Exemple dâ€™application :**  
> *Regrouper les villes de France selon leur climat et leur population.*  
> â†’ Si on coupe lâ€™arbre en **2 groupes**, on pourrait avoir **rÃ©gions urbaines vs rurales**.  
> â†’ Mais si on coupe en **6 groupes**, on pourrait distinguer **diffÃ©rentes zones climatiques et Ã©conomiques**.  

---

## ğŸ”— 2ï¸âƒ£ Algorithme **Divisif** (SÃ©paration Progressive)  

> **Approche descendante** : On commence avec **un seul grand groupe** contenant tous les individus et on le **divise progressivement** en clusters plus petits.  

ğŸ“Œ **Principe :**  
1. On part dâ€™un **groupe unique avec une forte hÃ©tÃ©rogÃ©nÃ©itÃ©**.  
2. On essaye de **trouver des sÃ©parations naturelles** dans les donnÃ©es.  
3. On continue jusquâ€™Ã  obtenir **des sous-groupes homogÃ¨nes**.  

ğŸ“Œ **DiffÃ©rence avec lâ€™algorithme agglomÃ©ratif**  
âœ… **AglomÃ©ratif** : On **assemble** progressivement des clusters.  
âœ… **Divisif** : On **divise** un groupe gÃ©nÃ©ral en sous-groupes.  

ğŸš€ **Pourquoi cet algorithme est rarement utilisÃ© pour le clustering ?**  
- Il est **complexe et coÃ»teux en calcul**.  
- Peu implÃ©mentÃ© en pratique pour la segmentation automatique.  
- Mais il est utilisÃ© en **classification supervisÃ©e**.  

ğŸ’¡ **Exemple dâ€™application en classification :**  
> *Dans le dÃ©pistage du cancer du sein, si une seule variable (ex : taille de la tumeur) permet de sÃ©parer **les patients malades et non malades**, alors le problÃ¨me est rÃ©solu.*  

---

## âš ï¸ InconvÃ©nients de la Classification HiÃ©rarchique  

âŒ **ProblÃ¨me de scalabilitÃ©**  
- Ne fonctionne **pas bien sur les grands jeux de donnÃ©es**.  
- **ComplexitÃ© Ã©levÃ©e** : Lâ€™algorithme doit comparer **toutes les paires dâ€™individus**, ce qui devient **trop long pour des milliers de points**.  

âœ… **Solution alternative :**  
ğŸ“Œ **Utiliser K-Means ou DBSCAN**, qui sont plus rapides et adaptÃ©s aux grands volumes de donnÃ©es.  

---

## ğŸ¯ **RÃ©sumÃ©**  

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">MÃ©thode</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Principe</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Utilisation</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">AglomÃ©ratif</td>
            <td style="padding: 10px; border: 1px solid #555;">Regroupe progressivement les individus les plus proches jusquâ€™Ã  former un unique cluster.</td>
            <td style="padding: 10px; border: 1px solid #555;">Clustering hiÃ©rarchique, segmentation de donnÃ©es.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">Divisif</td>
            <td style="padding: 10px; border: 1px solid #555;">SÃ©pare un grand groupe en sous-groupes de plus en plus petits.</td>
            <td style="padding: 10px; border: 1px solid #555;">Rare en clustering, mais utilisÃ© en classification.</td>
        </tr>
    </tbody>
</table>


# ğŸ”¹ K-Means (Lloyd)

Le **K-Means** est un **algorithme de clustering** qui regroupe les donnÃ©es en **K groupes homogÃ¨nes**. Il repose sur la notion de **barycentre** et fonctionne de maniÃ¨re itÃ©rative pour minimiser la distance intra-classe.  

---

## âš™ï¸ 1ï¸âƒ£ Fonctionnement de K-Means  

ğŸ“Œ **Ã‰tapes principales :**  
1. **DÃ©finir alÃ©atoirement** deux points de dÃ©part (appelÃ©s **centroÃ¯des**).  
2. **Calculer les distances** de chaque individu par rapport Ã  ces points.  
3. **DÃ©placer les centroÃ¯des** vers le **barycentre** des classes obtenues.  
4. **RÃ©pÃ©ter lâ€™opÃ©ration** jusquâ€™Ã  ce quâ€™il nâ€™y ait plus de variation entre deux itÃ©rations.  

ğŸ“Œ **Deux versions possibles :**  
- **DÃ©marrage avec 2 individus existants**.  
- **DÃ©marrage avec 2 points totalement alÃ©atoires**.  

ğŸ”´ **ProblÃ¨me possible :**  
Lâ€™algorithme peut **ne pas converger** si les classes ne sont pas bien dÃ©finies, ce qui peut le faire **tourner en boucle**.  

---

## ğŸ¯ 2ï¸âƒ£ DÃ©terminer le Nombre Optimal de Classes  

ğŸ“Œ **MÃ©thodes utilisÃ©es :**  
1ï¸âƒ£ **Tester toutes les classifications possibles** et comparer les rÃ©sultats.  
2ï¸âƒ£ **Utiliser lâ€™indice de silhouette** pour choisir la meilleure segmentation.  

ğŸ“Œ **MÃ©thode de la silhouette :**  
- Pour **chaque individu**, on calcule :  
  1. La **distance moyenne** entre lui-mÃªme et tous les individus de **sa propre classe**.  
  2. La **distance moyenne** entre lui-mÃªme et tous les individus dâ€™une **autre classe**.  
- On analyse si un individu **aurait Ã©tÃ© mieux placÃ© dans une autre classe**.  
- On cherche **oÃ¹ la distance est minimale** :  
  - **Si la distance intra-classe est plus faible** â†’ Bonne classification âœ…  
  - **Si la distance avec une autre classe est plus faible** â†’ Mauvaise classification âŒ  

---

## âš ï¸ 3ï¸âƒ£ Limites de la MÃ©thode de la Silhouette  

> **La silhouette ne fonctionne pas dans tous les cas !**  

ğŸ“Œ **Cas oÃ¹ elle Ã©choue :**  
- Lorsque les **groupes sâ€™emboÃ®tent** (ex: **des cercles imbriquÃ©s**).  
- Dans ces cas, un point en **bordure dâ€™un cercle** sera **plus proche du centre dâ€™un autre cercle** que dâ€™un point Ã©loignÃ© dans son propre cluster.  

ğŸ“Œ **Illustration :**  
![alt text](image-1.png)  

---

## ğŸ”„ 4ï¸âƒ£ K-Means pour la RÃ©duction de la ComplexitÃ©  

ğŸ“Œ **Autre usage important :**  
- **K-Means permet de rÃ©duire le nombre de valeurs** en diminuant la complexitÃ© des donnÃ©es.  
- **Principe** : En **crÃ©ant des clusters**, on peut **attribuer de nouveaux individus Ã  des groupes prÃ©dÃ©finis**.  
- **Avantage** :  
  - MÃªme si **aucune information nâ€™est crÃ©Ã©e**, on **ajoute artificiellement des valeurs**, ce qui amÃ©liore le comportement des algorithmes de classification.  

---

## ğŸ“Œ 5ï¸âƒ£ Normalisation des DonnÃ©es  

ğŸ’¡ **RÃ¨gle fondamentale :**  
âœ… **K-Means ne fonctionne bien que sur des colonnes centrÃ©es et rÃ©duites.**  
âœ” Cela Ã©vite quâ€™une **variable avec une Ã©chelle plus grande** (ex: **salaire en euros vs Ã¢ge en annÃ©es**) **domine le calcul des distances**.  



# ğŸŒ DBSCAN (Density-Based Spatial Clustering of Applications with Noise)  

Le **DBSCAN** est un algorithme de clustering qui identifie les groupes denses de points dans un espace tout en dÃ©tectant les **valeurs extrÃªmes** (outliers). Contrairement Ã  **K-Means**, il **ne nÃ©cessite pas de spÃ©cifier un nombre de clusters Ã  l'avance** et permet dâ€™identifier des groupes de formes variÃ©es.  

---

## âš™ï¸ 1ï¸âƒ£ Fonctionnement de DBSCAN  

ğŸ“Œ **ParamÃ¨tres essentiels :**  
- **Îµ (epsilon)** â†’ Rayon de la boule autour dâ€™un point.  
- **MinPts (nombre minimum dâ€™individus)** â†’ Nombre dâ€™individus requis dans la boule pour valider un cluster.  

ğŸ“Œ **Processus :**  
1. On place une **boule de rayon Îµ** autour d'un individu.  
2. Si le nombre dâ€™individus Ã  lâ€™intÃ©rieur de cette boule **est supÃ©rieur ou Ã©gal Ã  MinPts**, le point devient un **point central** dâ€™un cluster.  
3. Lâ€™algorithme continue en **expansant la rÃ©gion dense**, en ajoutant les individus voisins au cluster.  
4. **Si un point nâ€™a pas assez de voisins**, il est considÃ©rÃ© comme **un outlier (cas extrÃªme)**.  

ğŸ’¡ **Exemple d'exÃ©cution :**  
- Le point **A** forme un cluster car il a **3 voisins** dans son rayon Îµ.  
- Le point **C**, en revanche, **nâ€™a quâ€™un seul voisin**, donc il **nâ€™est pas inclus dans un cluster**.  

---

## ğŸš€ 2ï¸âƒ£ Avantages de DBSCAN  

âœ… **Identification des valeurs extrÃªmes**  
- Contrairement Ã  **K-Means**, DBSCAN **ne force pas tous les points Ã  appartenir Ã  un cluster**.  
- Les points isolÃ©s sont dÃ©tectÃ©s comme **outliers**, ce qui est utile pour lâ€™analyse dâ€™anomalies.  

âœ… **DÃ©tection de clusters de formes complexes**  
- DBSCAN peut identifier des **groupes de formes irrÃ©guliÃ¨res**, contrairement Ã  K-Means qui suppose des clusters sphÃ©riques.  

âœ… **Pas besoin de spÃ©cifier le nombre de clusters Ã  lâ€™avance**  
- Lâ€™algorithme **dÃ©cide automatiquement du nombre de clusters** en fonction de la densitÃ© locale.  

---

## âš ï¸ 3ï¸âƒ£ InconvÃ©nients et Limites  

âŒ **ProblÃ¨me du rÃ©glage des paramÃ¨tres (Îµ & MinPts)**  
- Une **lÃ©gÃ¨re variation** du **rayon Îµ** ou du **nombre minimal de points** peut entraÃ®ner **des rÃ©sultats trÃ¨s diffÃ©rents**.  
- DÃ©finir **Îµ trop grand** â†’ Les clusters fusionnent.  
- DÃ©finir **Îµ trop petit** â†’ Trop dâ€™outliers sont dÃ©tectÃ©s.  

âŒ **Difficile Ã  utiliser sur des jeux de donnÃ©es Ã  densitÃ© variable**  
- Si certaines rÃ©gions ont une **densitÃ© Ã©levÃ©e** et dâ€™autres une **densitÃ© faible**, lâ€™algorithme peut **ne pas bien segmenter** les clusters.  

ğŸ“Œ **Illustration du fonctionnement :**  
![alt text](image-2.png)  

---

## ğŸ¯ **RÃ©sumÃ© : Comparaison avec K-Means**  

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">CritÃ¨re</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">K-Means</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">DBSCAN</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">Type de clustering</td>
            <td style="padding: 10px; border: 1px solid #555;">BasÃ© sur les centres des clusters (centroÃ¯des).</td>
            <td style="padding: 10px; border: 1px solid #555;">BasÃ© sur la densitÃ© des points.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">Forme des clusters</td>
            <td style="padding: 10px; border: 1px solid #555;">PlutÃ´t circulaire ou sphÃ©rique.</td>
            <td style="padding: 10px; border: 1px solid #555;">Peut dÃ©tecter des formes complexes.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Gestion des valeurs extrÃªmes</td>
            <td style="padding: 10px; border: 1px solid #555;">Tous les points doivent Ãªtre dans un cluster.</td>
            <td style="padding: 10px; border: 1px solid #555;">Les outliers sont dÃ©tectÃ©s et exclus.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #6699ff;">DÃ©finition du nombre de clusters</td>
            <td style="padding: 10px; border: 1px solid #555;">Doit Ãªtre fixÃ© Ã  lâ€™avance.</td>
            <td style="padding: 10px; border: 1px solid #555;">DÃ©tectÃ© automatiquement.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff9966;">DifficultÃ©</td>
            <td style="padding: 10px; border: 1px solid #555;">Simple et rapide.</td>
            <td style="padding: 10px; border: 1px solid #555;">SensibilitÃ© au choix des paramÃ¨tres.</td>
        </tr>
    </tbody>
</table>

---
# ğŸ¯ Les Algorithmes SupervisÃ©s  

Lâ€™apprentissage **supervisÃ©** consiste Ã  entraÃ®ner un modÃ¨le sur un **jeu de donnÃ©es Ã©tiquetÃ©**, afin qu'il puisse **prÃ©dire une classe ou une valeur** Ã  partir de nouvelles donnÃ©es.  

---

## ğŸ” 1ï¸âƒ£ Les Plus Proches Voisins (K-Nearest Neighbors - KNN)  

Le **KNN** est un **algorithme de classification** basÃ© sur la **proximitÃ©** entre les points. Il attribue **une classe** Ã  un nouvel individu en fonction des **K voisins les plus proches**.  

---

## âš™ï¸ 2ï¸âƒ£ Fonctionnement de KNN  

ğŸ“Œ **Principe :**  
1. DÃ©finir un **nombre de voisins K**.  
2. Mesurer la **distance** entre le nouvel individu et **tous les autres points** du dataset.  
3. SÃ©lectionner les **K voisins les plus proches**.  
4. Lâ€™individu est **classÃ© dans la catÃ©gorie majoritaire** parmi ces voisins.  

ğŸ“Œ **Illustration :**  
- Si **K = 3**, on prend les **3 points les plus proches** et on vote pour la classe majoritaire.  
- Si **K = 5**, on prend les **5 plus proches** voisins.  

ğŸ’¡ **Pourquoi est-ce utile ?**  
ğŸ‘‰ **Plus K est petit**, plus l'algorithme est sensible au **bruit** et aux outliers.  
ğŸ‘‰ **Plus K est grand**, plus la classification est **robuste**, mais moins prÃ©cise.  

---

## ğŸš€ 3ï¸âƒ£ Avantages de KNN  

âœ… **Simple et intuitif**  
- Facile Ã  comprendre et Ã  implÃ©menter.  
- Pas besoin dâ€™entraÃ®ner un modÃ¨le complexe.  

âœ… **AdaptÃ© aux petits jeux de donnÃ©es**  
- Fonctionne bien lorsque le nombre dâ€™Ã©chantillons est **faible**.  

âœ… **Pas besoin d'hypothÃ¨ses sur les donnÃ©es**  
- Contrairement Ã  la rÃ©gression linÃ©aire ou logistique, il **ne suppose pas** de relation mathÃ©matique entre les variables.  

---

## âš ï¸ 4ï¸âƒ£ InconvÃ©nients et Limites  

âŒ **Lent pour les grandes bases de donnÃ©es**  
- **Chaque nouvelle prÃ©diction** nÃ©cessite de **calculer la distance avec tous les points**, ce qui peut Ãªtre **trÃ¨s coÃ»teux** en temps de calcul.  

âŒ **SensibilitÃ© au choix du paramÃ¨tre K**  
- **Un mauvais choix de K** peut impacter la prÃ©cision du modÃ¨le.  

âŒ **Ne fonctionne pas bien si les donnÃ©es ne sont pas bien sÃ©parables**  
- Si les **classes se mÃ©langent**, KNN peut **se tromper frÃ©quemment**.  

---

## ğŸ¯ **RÃ©sumÃ© : Avantages et InconvÃ©nients de KNN**  

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">CritÃ¨re</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Avantages</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">InconvÃ©nients</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">FacilitÃ© dâ€™implÃ©mentation</td>
            <td style="padding: 10px; border: 1px solid #555;">Simple, intuitif, pas besoin dâ€™entraÃ®nement.</td>
            <td style="padding: 10px; border: 1px solid #555;">Lent sur de grands datasets.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">Performance</td>
            <td style="padding: 10px; border: 1px solid #555;">Bon pour les petits jeux de donnÃ©es.</td>
            <td style="padding: 10px; border: 1px solid #555;">Inefficace si les donnÃ©es sont mal sÃ©parÃ©es.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">ParamÃ©trage</td>
            <td style="padding: 10px; border: 1px solid #555;">Ne nÃ©cessite pas d'hypothÃ¨ses sur les donnÃ©es.</td>
            <td style="padding: 10px; border: 1px solid #555;">Le choix de K est critique.</td>
        </tr>
    </tbody>
</table>

---

## La rÃ©gression linÃ©aire 

### DÃ©finition

    La rÃ©gression est une mÃ©thode de prÃ©diction utilisÃ©e en Machine Learning pour estimer une valeur numÃ©rique en fonction dâ€™autres variables.

ğŸ‘‰ Objectif : Trouver une relation entre une ou plusieurs variables dâ€™entrÃ©e (features) et une variable cible.


### ğŸ“Œ Quand utiliser la rÃ©gression linÃ©aire ?

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Cas dâ€™utilisation</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Explication</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">PrÃ©dire une valeur continue</td>
            <td style="padding: 10px; border: 1px solid #555;">Prix dâ€™une maison, salaire, tempÃ©rature.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">Relation linÃ©aire entre les variables</td>
            <td style="padding: 10px; border: 1px solid #555;">Si augmenter <b>X</b> augmente <b>Y</b> de maniÃ¨re proportionnelle.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Peu de donnÃ©es, besoin dâ€™un modÃ¨le simple</td>
            <td style="padding: 10px; border: 1px solid #555;">ModÃ¨le rapide et interprÃ©table.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #6699ff;">Comprendre quelles variables influencent Y</td>
            <td style="padding: 10px; border: 1px solid #555;">Identifier les variables importantes avant dâ€™utiliser un modÃ¨le plus complexe.</td>
        </tr>
    </tbody>
</table>

âœ… Elle optimise les autres algorithmes en aidant Ã  :

    âœ” SÃ©lectionner les meilleures variables.
    âœ” Tester si une relation linÃ©aire existe avant dâ€™appliquer un modÃ¨le complexe.
    âœ” Servir de base mathÃ©matique pour dâ€™autres modÃ¨les (logistique, SVM)


## La rÃ©gression logistique 

La rÃ©gression logistique est un algorithme de classification qui permet de prÃ©dire une catÃ©gorie Ã  partir de donnÃ©es. Contrairement Ã  la rÃ©gression linÃ©aire (qui prÃ©dit une valeur continue), la rÃ©gression logistique prÃ©dit une probabilitÃ© et attribue une classe (ex: "Oui" ou "Non", "Spam" ou "Non-Spam", etc.).

ğŸ’¡ Exemples concrets : 

    âœ” DÃ©tecter un spam : Est-ce quâ€™un email est "Spam" ou "Non Spam" ?

    âœ” Diagnostic mÃ©dical : Un patient est-il malade (1) ou en bonne santÃ© (0) ?

    âœ” PrÃ©diction dâ€™achat : Un utilisateur va-t-il acheter un produit (Oui/Non) ?


âœ… Avantages :

    âœ” Simple Ã  comprendre et rapide Ã  entraÃ®ner.
    âœ” Fournit une probabilitÃ©, utile pour ajuster les seuils.
    âœ” Fonctionne bien sur des petits jeux de donnÃ©es.

âŒ Limites :

    âŒ Ne fonctionne que pour deux catÃ©gories (mais il existe des extensions pour plusieurs classes).
    âŒ Suppose que les donnÃ©es sont bien sÃ©parables (si ce nâ€™est pas le cas, un SVM ou un rÃ©seau de neurones est plus adaptÃ©).

## Les arbres de dÃ©cision  

ğŸ’¡ Comment Ã§a marche ?

- Lâ€™algorithme analyse colonne par colonne pour identifier celle qui sÃ©pare le mieux les donnÃ©es selon la variable cible (ex: espÃ¨ce dâ€™un animal, achat ou non dâ€™un client).
- Il construit ensuite une arborescence avec des questions successives qui mÃ¨nent Ã  une dÃ©cision finale.

ğŸ“Œ ParamÃ©trage :

- On peut limiter la profondeur de lâ€™arbre pour Ã©viter le sur-apprentissage.
- Un arbre trop profond est trop prÃ©cis sur lâ€™entraÃ®nement mais gÃ©nÃ©ralise mal.

ğŸ“Œ ProblÃ¨mes :

    1ï¸âƒ£ Sur-apprentissage : Trop de dÃ©coupages, ne fonctionne bien que sur lâ€™entraÃ®nement.

    2ï¸âƒ£ Prise de dÃ©cision une variable Ã  la fois : Ne combine pas plusieurs colonnes en mÃªme temps.

    3ï¸âƒ£ Gestion des variables qualitatives : Besoin de transformer les catÃ©gories en valeurs numÃ©riques.


ğŸ“Œ Comment rÃ©duire le sur-apprentissage ?

- Limiter la profondeur de lâ€™arbre.
- MÃ©thodes dâ€™ensemble (Ensemble Learning) :
    - Tree Bagging (moyenne plusieurs arbres).
    - Random Forest (arbres construits en parallÃ¨le).
    - Boosting (arbres construits en sÃ©rie, plus lent mais plus prÃ©cis).


 # ğŸ¯ Apprentissage en Machine Learning  

L'apprentissage en Machine Learning consiste Ã  **entraÃ®ner un modÃ¨le** pour qu'il puisse **faire des prÃ©dictions prÃ©cises** sur de nouvelles donnÃ©es.  

ğŸ“Œ **ProblÃ¨me central :**  
Il faut trouver un **Ã©quilibre** entre **un modÃ¨le trop simple** (qui ne capture pas bien les tendances) et **un modÃ¨le trop complexe** (qui sâ€™adapte trop aux donnÃ©es dâ€™entraÃ®nement mais gÃ©nÃ©ralise mal).  

---

## âš™ï¸ 1ï¸âƒ£ SÃ©parer les DonnÃ©es en Apprentissage & Test  

ğŸ“Œ **Pourquoi ?**  
- **Ã‰valuer la performance rÃ©elle de notre modÃ¨le**.  
- **VÃ©rifier s'il peut bien gÃ©nÃ©raliser** Ã  de nouvelles donnÃ©es.  

ğŸ“Œ **Comment faire ?**  
1. **DÃ©couper le dataset** en deux parties :  
   - **Ensemble d'apprentissage (Training Set) ğŸ‹ï¸â€â™‚ï¸** : UtilisÃ© pour **entraÃ®ner le modÃ¨le**.  
   - **Ensemble de test (Test Set) ğŸ§ª** : UtilisÃ© pour **Ã©valuer sa performance**.  
2. **Optionnel : Un ensemble de validation** pour ajuster les hyperparamÃ¨tres.  

ğŸ’¡ **Bonne pratique :**  
GÃ©nÃ©ralement, on utilise **80% des donnÃ©es pour lâ€™entraÃ®nement** et **20% pour le test**.  

---

## ğŸ¯ 2ï¸âƒ£ ComplexitÃ© et Sur-Apprentissage  

ğŸ“Œ **La complexitÃ© du modÃ¨le :**  
- Un modÃ¨le **simple** (ex: une droite) peut **ne pas capturer toutes les tendances**.  
- Un modÃ¨le **trop complexe** (ex: une courbe qui passe par tous les points) peut **mÃ©moriser chaque point mais ne pas bien gÃ©nÃ©raliser**.  

ğŸ“Œ **Impact de la complexitÃ© :**  
âœ” **Augmenter la complexitÃ© dâ€™un modÃ¨le** peut conduire Ã  **un taux de rÃ©ussite de 100% sur les donnÃ©es dâ€™entraÃ®nement**.  
âŒ Mais cela **ne garantit pas de bonnes prÃ©dictions** sur de nouvelles donnÃ©es.  

---

## ğŸ”„ 3ï¸âƒ£ GÃ©nÃ©ralisation du ModÃ¨le  

> **Un bon modÃ¨le ne doit pas juste apprendre les donnÃ©es dâ€™entraÃ®nement, mais aussi Ãªtre capable de gÃ©nÃ©raliser.**  

ğŸ“Œ **Si le modÃ¨le est trop complexe** â†’ Il apprend **chaque dÃ©tail des donnÃ©es dâ€™entraÃ®nement** mais **ne sait pas sâ€™adapter aux nouvelles donnÃ©es**.  

ğŸ“Œ **Si le modÃ¨le est trop simple** â†’ Il **ne capture pas bien les tendances** et fait des prÃ©dictions mÃ©diocres.  

ğŸ’¡ **Lâ€™objectif est de trouver un Ã©quilibre** entre **biais et variance**.  

---

## ğŸ“Š 4ï¸âƒ£ Sur-Apprentissage (Overfitting) vs Sous-Apprentissage (Underfitting)  

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Type dâ€™apprentissage</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Biais</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Variance</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">ConsÃ©quence</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Sur-Apprentissage (Overfitting) ğŸ”´</td>
            <td style="padding: 10px; border: 1px solid #555; color: #66ff99; font-weight: bold;">Faible âœ…</td>
            <td style="padding: 10px; border: 1px solid #555; color: #ff6666; font-weight: bold;">Ã‰levÃ©e âŒ</td>
            <td style="padding: 10px; border: 1px solid #555;">Le modÃ¨le sâ€™adapte trop aux donnÃ©es dâ€™apprentissage mais <b>ne gÃ©nÃ©ralise pas</b>.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #6699ff;">Sous-Apprentissage (Underfitting) ğŸ”µ</td>
            <td style="padding: 10px; border: 1px solid #555; color: #ff6666; font-weight: bold;">Ã‰levÃ© âŒ</td>
            <td style="padding: 10px; border: 1px solid #555; color: #66ff99; font-weight: bold;">Faible âœ…</td>
            <td style="padding: 10px; border: 1px solid #555;">Le modÃ¨le est trop simple et <b>ne capture pas les tendances</b> des donnÃ©es.</td>
        </tr>
    </tbody>
</table>

ğŸ“Œ **IdÃ©alement, on cherche un modÃ¨le avec un compromis entre biais et variance**.  

---

## ğŸ“Œ 5ï¸âƒ£ Comment Ã‰viter le Sur-Apprentissage ?  

âœ… **Utiliser un ensemble de test** : Permet de **vÃ©rifier si le modÃ¨le gÃ©nÃ©ralise bien**.  
âœ… **Limiter la complexitÃ© du modÃ¨le** : Ã‰viter un **trop grand nombre de paramÃ¨tres**.  
âœ… **RÃ©gularisation (L1, L2)** : Ajoute une **pÃ©nalitÃ© sur les modÃ¨les trop complexes**.  
âœ… **Cross-validation** : Tester le modÃ¨le sur **plusieurs sous-Ã©chantillons des donnÃ©es**.  
âœ… **Plus de donnÃ©es !** : Ajouter plus de donnÃ©es **aide Ã  Ã©viter le sur-apprentissage**.  


# ğŸ¯ L'Ã‰chantillonnage en Machine Learning  

Lâ€™Ã©chantillonnage est une **Ã©tape essentielle** en Machine Learning pour **sÃ©parer les donnÃ©es** et garantir un **Ã©quilibre entre les classes**.  

---

## âš–ï¸ 1ï¸âƒ£ Stratification  

ğŸ“Œ **DÃ©finition :**  
La **stratification** permet de s'assurer que le **dÃ©coupage des donnÃ©es** (train/test) soit **Ã©quilibrÃ©** par rapport Ã  une variable.  

ğŸ“Œ **Pourquoi l'utiliser ?**  
- Ã‰vite dâ€™avoir un **dÃ©sÃ©quilibre** dans la distribution des classes.  
- AmÃ©liore la **reprÃ©sentativitÃ©** des donnÃ©es d'entraÃ®nement et de test.  

âš ï¸ **Attention !**  
La variable Ã  **stratifier doit Ãªtre qualitative** (ex: catÃ©gories comme "spam / non-spam", "malade / sain").  

ğŸ’¡ **Exemple d'utilisation :**  
> Si on entraÃ®ne un modÃ¨le de classification sur des patients avec 80% de malades et 20% de non-malades, on veut que cette **proportion soit la mÃªme** dans lâ€™ensemble de test.  

---

## ğŸ”„ 2ï¸âƒ£ PrÃ©diction CroisÃ©e (Ensemble Learning)  

ğŸ“Œ **ProblÃ¨me :**  
Comment **combiner plusieurs modÃ¨les** pour obtenir une meilleure prÃ©diction ?  

ğŸ“Œ **Solution :**  
On utilise la **prÃ©diction croisÃ©e** pour **agrÃ©ger les rÃ©sultats de plusieurs modÃ¨les** et amÃ©liorer la robustesse des dÃ©cisions.  

ğŸ“Œ **Principe :**  
1. On entraÃ®ne **5 modÃ¨les diffÃ©rents**.  
2. Chaque modÃ¨le renvoie **deux informations** :  
   - **La prÃ©diction** (0 ou 1 en classification binaire).  
   - **La probabilitÃ© associÃ©e** Ã  cette prÃ©diction.  
3. On **additionne les probabilitÃ©s** fournies par chaque modÃ¨le.  
4. On choisit la **classe majoritaire** ou on applique un **seuil de dÃ©cision**.  

ğŸ’¡ **Exemple :**  
> Si 5 modÃ¨les donnent chacun une probabilitÃ© pour la classe "Spam" :  
> - ModÃ¨le 1 â†’ 0.72  
> - ModÃ¨le 2 â†’ 0.80  
> - ModÃ¨le 3 â†’ 0.65  
> - ModÃ¨le 4 â†’ 0.90  
> - ModÃ¨le 5 â†’ 0.75  
>  
> On **fait la moyenne** :  
> **(0.72 + 0.80 + 0.65 + 0.90 + 0.75) / 5 = 0.76**  
> Si le seuil est **0.7**, alors lâ€™email est classÃ© comme **Spam** âœ…  

---

## ğŸ“Š DiffÃ©rence entre Stratification et PrÃ©diction CroisÃ©e  

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Concept</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">DÃ©finition</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Utilisation principale</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">Stratification ğŸ·ï¸</td>
            <td style="padding: 10px; border: 1px solid #555;">DÃ©coupage Ã©quilibrÃ© des donnÃ©es selon une variable qualitative.</td>
            <td style="padding: 10px; border: 1px solid #555;">Assurer un Ã©chantillonnage reprÃ©sentatif pour lâ€™entraÃ®nement et le test.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">PrÃ©diction CroisÃ©e ğŸ”„</td>
            <td style="padding: 10px; border: 1px solid #555;">Combiner plusieurs modÃ¨les en moyennant leurs prÃ©dictions.</td>
            <td style="padding: 10px; border: 1px solid #555;">AmÃ©liorer la robustesse et la fiabilitÃ© des dÃ©cisions dâ€™un modÃ¨le.</td>
        </tr>
    </tbody>
</table>

---

## ğŸ¯ **RÃ©sumÃ© : Pourquoi ces techniques sont importantes ?**  

âœ… **La stratification garantit un dÃ©coupage Ã©quilibrÃ© des donnÃ©es**, Ã©vitant les biais liÃ©s Ã  une classe dominante.  
âœ… **La prÃ©diction croisÃ©e permet dâ€™agrÃ©ger plusieurs modÃ¨les**, ce qui amÃ©liore la prÃ©cision et rÃ©duit les erreurs.  

ğŸ“Œ **En combinant ces deux techniques, on obtient un modÃ¨le plus fiable et robuste.** ğŸš€  

---


# Choix des mÃ©trique de performance 

## Classification (qualitatif)

### Matrice de confusion 

Cette matrice permet d'analyser les performances d'un modÃ¨le de classification en regardant oÃ¹ il fait des erreurs et en Ã©valuant la pertinence des prÃ©dictions

![alt text](image-5.png)

Exemple d'application : 

- Diagnostic mÃ©dical
- DÃ©tection de spam
- Reconnaissance faciale

### La courbe Roc 
#### DÃ©finition 

La courbe ROC (Receiver Operating Characteristic) est un outil permettant d'Ã©valuer la performance dâ€™un modÃ¨le de classification binaire.

ğŸ’¡ Pourquoi lâ€™utiliser ?

Elle mesure la capacitÃ© du modÃ¨le Ã  bien classifier les classes positives et nÃ©gatives.
Elle permet dâ€™analyser lâ€™impact du seuil de dÃ©cision sur les performances du modÃ¨le.
L'aire sous la courbe (AUC - Area Under Curve) donne un indicateur global de la qualitÃ© du modÃ¨le

#### ğŸ“Œ Les Axes de la Courbe ROC

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Axe</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Explication</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">Axe Y (ordonnÃ©e) = SensibilitÃ© (Recall)</td>
            <td style="padding: 10px; border: 1px solid #555;">Proportion de <b>vrais positifs dÃ©tectÃ©s</b> par rapport Ã  tous les positifs rÃ©els.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Axe X (abscisse) = Taux de faux positifs (1 - SpÃ©cificitÃ©)</td>
            <td style="padding: 10px; border: 1px solid #555;">Proportion de <b>faux positifs dÃ©tectÃ©s</b> par rapport Ã  tous les nÃ©gatifs rÃ©els.</td>
        </tr>
    </tbody>
</table>


#### ğŸ“Œ InterprÃ©tation d'un Point sur la Courbe ROC

- Chaque point de la courbe ROC correspond Ã  un seuil de classification diffÃ©rent.
- Le seuil de probabilitÃ© permet de dÃ©cider Ã  partir de quelle probabilitÃ© on classe une observation comme "positive".
- Si on diminue le seuil, on dÃ©tecte plus de vrais positifs mais aussi plus de faux positifs.
- Si on augmente le seuil, on rÃ©duit les faux positifs, mais on risque de manquer des vrais positifs.
Exemple ğŸ“Œ :

Seuil = 50% â†’ Le modÃ¨le classe "positif" si la probabilitÃ© est â‰¥ 50%.
Seuil = 70% â†’ On devient plus strict, donc moins de faux positifs mais plus de faux nÃ©gatifs.

#### ğŸ“Œ L'Aire Sous la Courbe (AUC - Area Under Curve)

L'AUC (Area Under the Curve) reprÃ©sente la probabilitÃ© quâ€™un modÃ¨le classe un exemple positif avant un nÃ©gatif.

- AUC = 1 â†’ ModÃ¨le parfait (sÃ©pare parfaitement les classes).
- AUC = 0.5 â†’ ModÃ¨le alÃ©atoire (aucune capacitÃ© de classification).
- AUC < 0.5 â†’ ModÃ¨le inversÃ© (prÃ©dit le contraire de la vÃ©ritÃ©).



## ğŸŸ¢ RÃ©gression (Quantitatif)

---

### ğŸ”¹ CorrÃ©lation de Pearson (Quantitatif)

> **Mesure la relation linÃ©aire entre deux variables numÃ©riques.**  

ğŸ“Œ **Pourquoi lâ€™utiliser ?**  
Lorsquâ€™on entraÃ®ne un modÃ¨le de rÃ©gression, on cherche Ã  prÃ©dire une variable continue (ex: **prix dâ€™une maison, tempÃ©rature, chiffre d'affaires**).  

ğŸ“ˆ **InterprÃ©tation du coefficient de Pearson (ğ‘Ÿ) :**  
- ğŸŸ¢ **ğ‘Ÿ proche de 1** â†’ Le modÃ¨le suit **bien** la tendance des donnÃ©es rÃ©elles.  
- âšª **ğ‘Ÿ proche de 0** â†’ Aucune **corrÃ©lation linÃ©aire** entre les prÃ©dictions et les valeurs rÃ©elles (**modÃ¨le peu fiable**).  
- ğŸ”´ **ğ‘Ÿ nÃ©gatif** â†’ Les prÃ©dictions sont **Ã  l'opposÃ©** des valeurs rÃ©elles (**erreur systÃ©matique**).  

âœ… En complÃ©ment des **erreurs absolues** (comme **RMSE** et **MAE**), la corrÃ©lation de Pearson permet de **vÃ©rifier si le modÃ¨le suit la bonne dynamique gÃ©nÃ©rale**.  

---

### ğŸ”¹ RSS (Residual Sum of Squares)

> **Le rÃ©sidu reprÃ©sente l'erreur pour chacune des prÃ©dictions.**  

ğŸ“Œ **DÃ©finition :**  
Le **RSS** est la somme des erreurs **Ã©levÃ©es au carrÃ©**. Plus le RSS est **faible**, plus le modÃ¨le **ajuste bien les donnÃ©es**.  

âš  **Limite :**  
âŒ **Ne permet pas de comparer** des modÃ¨les entre diffÃ©rents jeux de donnÃ©es, car **sa valeur dÃ©pend de la taille de lâ€™Ã©chantillon**.  

---

### ğŸ”¹ MSE (Mean Squared Error)

âœ… **UtilitÃ© :**  
- Permet de **comparer les performances** dâ€™un modÃ¨le entre **diffÃ©rents jeux de test ou algorithmes**.  
- Aide Ã  **choisir la meilleure optimisation** ou le **meilleur kernel** (ex: en **SVM** ou **rÃ©gression**).  

âš  **ProblÃ¨me :**  
âŒ Lâ€™unitÃ© du **MSE** **nâ€™est pas la mÃªme** que celle de la variable cible **ğ‘Œ** â—  

---

### ğŸ”¹ RMSE (Root Mean Squared Error)

âœ… **Pourquoi utiliser le RMSE ?**  
- Il permet dâ€™avoir une **erreur interprÃ©table** car elle est **dans la mÃªme unitÃ© que ğ‘Œ**.  
- TrÃ¨s utile pour **comparer des modÃ¨les de rÃ©gression** et voir lâ€™**ampleur des erreurs en unitÃ©s rÃ©elles**.  

âš  **Limite :**  
âŒ **Ne fonctionne PAS pour des prÃ©dictions qualitatives** ! (Uniquement utilisÃ© en **rÃ©gression** et **non en classification**).  


## Kernel 

Un kernel est une fonction mathÃ©matique qui transforme les donnÃ©es pour les rendre sÃ©parables linÃ©airement dans un espace de plus haute dimension.

ğŸ“Œ Utilisation principale :
Il permet dâ€™utiliser des algorithmes linÃ©aires (comme SVM ou rÃ©gression) sur des donnÃ©es non linÃ©aires, en Ã©vitant de calculer explicitement toutes les dimensions.

âš  Si on choisit le mauvais kernel, la transformation peut rendre les donnÃ©es encore plus complexes, ce qui dÃ©grade la performance du modÃ¨le.

ğŸ‘‰ Cela permet d'Ã©conomiser du temps de calcul et de conserver des algorithmes optimisÃ©s tout en traitant des problÃ¨mes complexes.


<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Concept</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Explication</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">DÃ©finition</td>
            <td style="padding: 10px; border: 1px solid #555;">Le kernel transforme les donnÃ©es pour les rendre <b>sÃ©parables linÃ©airement</b>.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">Pourquoi ?</td>
            <td style="padding: 10px; border: 1px solid #555;">Quand les donnÃ©es sont <b>non linÃ©aires</b>, il permet dâ€™utiliser un <b>modÃ¨le linÃ©aire</b> dans un <b>espace transformÃ©</b>.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Exemples de Kernel</td>
            <td style="padding: 10px; border: 1px solid #555;">
                <b>LinÃ©aire</b> (pas de transformation) <br>
                <b>Polynomial</b> (tordre lâ€™espace en puissance) <br>
                <b>Radial (RBF)</b> (sÃ©parer des donnÃ©es circulaires)
            </td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #6699ff;">Avantage</td>
            <td style="padding: 10px; border: 1px solid #555;">Ã‰vite de <b>calculer toutes les dimensions</b>, permet dâ€™utiliser des <b>algorithmes linÃ©aires</b> sur des <b>donnÃ©es complexes</b>.</td>
        </tr>
    </tbody>
</table>



![alt text](image-4.png)


![alt text](image-3.png)




## La fonction de coÃ»t 

La fonction de coÃ»t est un outil utilisÃ© en Machine Learning pour mesurer l'erreur d'un modÃ¨le.

ğŸ‘‰ Elle sert Ã  Ã©valuer Ã  quel point les prÃ©dictions du modÃ¨le sont proches ou Ã©loignÃ©es des vraies valeurs.

    1ï¸âƒ£ Le modÃ¨le fait une prÃ©diction.
    2ï¸âƒ£ On compare cette prÃ©diction avec la valeur rÃ©elle.
    3ï¸âƒ£ La fonction de coÃ»t attribue une "pÃ©nalitÃ©" en fonction de lâ€™Ã©cart entre les deux.
    4ï¸âƒ£ Le modÃ¨le ajuste ses paramÃ¨tres pour rÃ©duire cette erreur et amÃ©liorer ses futures prÃ©dictions.



<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Concept</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Explication</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">DÃ©finition</td>
            <td style="padding: 10px; border: 1px solid #555;">Mesure l'erreur entre la prÃ©diction du modÃ¨le et la valeur rÃ©elle.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">UtilitÃ©</td>
            <td style="padding: 10px; border: 1px solid #555;">Permet dâ€™entraÃ®ner le modÃ¨le et dâ€™amÃ©liorer ses performances.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff6666;">Fonctionnement</td>
            <td style="padding: 10px; border: 1px solid #555;">Compare la prÃ©diction avec la vraie valeur et attribue une pÃ©nalitÃ© (erreur).</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ff9966;">Optimisation</td>
            <td style="padding: 10px; border: 1px solid #555;">Les algorithmes comme la <b>descente de gradient</b> minimisent cette erreur.</td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #6699ff;">Types</td>
            <td style="padding: 10px; border: 1px solid #555;">RÃ©gression : MSE (erreur quadratique) / Classification : Log-Loss (entropie croisÃ©e).</td>
        </tr>
    </tbody>
</table>


# ğŸ“‰ La Descente de Gradient en Machine Learning  

## ğŸ”¹ 1ï¸âƒ£ Quâ€™est-ce que la Descente de Gradient ?  

La **descente de gradient** est une technique d'optimisation utilisÃ©e en **Machine Learning** et **Deep Learning** pour **ajuster les paramÃ¨tres dâ€™un modÃ¨le** et **minimiser lâ€™erreur**.  

ğŸ’¡ **L'objectif :** Trouver les **meilleurs paramÃ¨tres** dâ€™un modÃ¨le afin quâ€™il fasse **les prÃ©dictions les plus prÃ©cises possibles**.  

---

## âš™ï¸ 2ï¸âƒ£ Comment fonctionne la Descente de Gradient ?  

ğŸ“Œ **Principe gÃ©nÃ©ral :**  
1. Le modÃ¨le commence avec **des paramÃ¨tres initiaux alÃ©atoires**.  
2. Il **calcule lâ€™erreur** entre la prÃ©diction et la vraie valeur.  
3. Il **ajuste progressivement les paramÃ¨tres** pour **rÃ©duire lâ€™erreur**, en suivant la pente du terrain des erreurs (le gradient).  
4. Ce processus se rÃ©pÃ¨te jusquâ€™Ã  **trouver une valeur optimale**.  

ğŸ’¡ **Analogie :**  
> Imaginez une **balle roulant sur une montagne**. Elle cherche **le point le plus bas** en descendant progressivement la pente. La descente de gradient suit le mÃªme principe : **trouver le minimum de lâ€™erreur**.  

---

## ğŸš€ 3ï¸âƒ£ Types de Descente de Gradient  

<table style="border: 1px solid #444; border-collapse: collapse; width: 100%; color: #fff; background-color: #222;">
    <thead>
        <tr style="background-color: #444;">
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Type</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Description</th>
            <th style="text-align: left; padding: 10px; border: 1px solid #555;">Avantages & InconvÃ©nients</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #ffcc00;">Batch Gradient Descent ğŸ‹ï¸â€â™‚ï¸</td>
            <td style="padding: 10px; border: 1px solid #555;">Met Ã  jour les paramÃ¨tres aprÃ¨s avoir analysÃ© <b>toutes</b> les donnÃ©es.</td>
            <td style="padding: 10px; border: 1px solid #555;">
                âœ… Plus stable ğŸ’¡<br>
                âŒ Plus lent sur de grands datasets.
            </td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #66ff99;">Stochastic Gradient Descent (SGD) âš¡</td>
            <td style="padding: 10px; border: 1px solid #555;">Met Ã  jour les paramÃ¨tres aprÃ¨s <b>chaque</b> point de donnÃ©es.</td>
            <td style="padding: 10px; border: 1px solid #555;">
                âœ… Rapide ğŸ’¡<br>
                âŒ Plus de variations (moins stable).
            </td>
        </tr>
        <tr>
            <td style="padding: 10px; border: 1px solid #555; font-weight: bold; color: #6699ff;">Mini-Batch Gradient Descent ğŸ¯</td>
            <td style="padding: 10px; border: 1px solid #555;">Met Ã  jour les paramÃ¨tres aprÃ¨s <b>un petit Ã©chantillon</b> de donnÃ©es.</td>
            <td style="padding: 10px; border: 1px solid #555;">
                âœ… Compromis entre stabilitÃ© et rapiditÃ©.
            </td>
        </tr>
    </tbody>
</table>


---

## âš ï¸ 4ï¸âƒ£ ProblÃ¨mes et Solutions  

ğŸ“Œ **1. Convergence trop lente â³**  
ğŸ’¡ **Solution** : Utiliser un **taux dâ€™apprentissage dynamique** (learning rate).  

ğŸ“Œ **2. Blocage dans un minimum local âŒ**  
ğŸ’¡ **Solution** : Utiliser des techniques comme **Momentum** ou **Adam Optimizer** pour mieux explorer lâ€™espace des solutions.  

ğŸ“Œ **3. Oscillations trop fortes ğŸ”„**  
ğŸ’¡ **Solution** : RÃ©gler correctement le **learning rate** pour Ã©viter des sauts trop brusques.  

---

## ğŸ¯ 5ï¸âƒ£ Pourquoi la Descente de Gradient est Importante ?  

âœ… **Essentielle pour lâ€™apprentissage des modÃ¨les de Machine Learning**  
âœ… **UtilisÃ©e dans la majoritÃ© des algorithmes de rÃ©gression et de rÃ©seaux neuronaux**  
âœ… **Permet dâ€™entraÃ®ner des modÃ¨les efficacement mÃªme sur des millions de donnÃ©es**  

---

## ğŸ“Œ 6ï¸âƒ£ RÃ©sumÃ©  

- **Descente de Gradient = Technique dâ€™optimisation** pour **rÃ©duire lâ€™erreur** dâ€™un modÃ¨le.  
- **Trois types principaux :** Batch, Stochastique (SGD), Mini-Batch.  
- **ProblÃ¨mes courants** : apprentissage trop lent, oscillations, minimum local.  
- **Techniques avancÃ©es** : Momentum, Adam, Learning Rate Adaptatif.  

ğŸ“Œ **Sans descente de gradient, lâ€™IA et le Machine Learning ne pourraient pas fonctionner efficacement ! ğŸš€**  
